import logging
from typing import List

from openai import OpenAI

from ingest.config.env import OPENAI_API_KEY, EMBEDDING_MODEL, EMBED_DIM

# Configure OpenAI client with API key
client = OpenAI(api_key=OPENAI_API_KEY)

# Configure logging
logger = logging.getLogger(__name__)

"""
    [CLASS] EmbeddingService: Text embedding service for generating vector representations of text.
    
    [DESCRIPTION]: 
        This class provides methods to convert text into high-dimensional vector
        representations that capture semantic meaning. It supports both single
        text embedding generation and batch processing for efficiency.
    
    [ATTRIBUTES]:
        EMBEDDING_MODEL (str): The OpenAI model used for embedding generation.
        EMBED_DIM (int): The dimension of the embeddings generated by the model.
    
    [METHODS]:
        generate_embedding(text: str) -> List[float]: Generates a single text embedding.
"""
class EmbeddingService:

    """
        [METHOD] generate_embedding: Generate a single text embedding.
        
        [DESCRIPTION]: 
            Converts a text string into a high-dimensional vector representation
            that captures semantic meaning. The embedding can be used for semantic
            search, similarity comparisons, and other NLP tasks.
    """
    @staticmethod
    def generate_embedding(text: str) -> List[float]:
        try:
            response = client.embeddings.create(model=EMBEDDING_MODEL,
            input=text)
            embedding = response.data[0].embedding

            if len(embedding) != EMBED_DIM:
                logger.warning(f"Embedding dimension mismatch: {len(embedding)} != {EMBED_DIM}")

            return embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise

    """
        [METHOD] batch_generate_embeddings: Generate embeddings for multiple texts.
        
        [DESCRIPTION]: 
            Generates embeddings for multiple texts efficiently in a single API call.
            This method is more efficient than calling generate_embedding() multiple
            times as it batches all texts into a single API request. Useful for
            processing large datasets or multiple documents at once.
    """
    @staticmethod
    def batch_generate_embeddings(texts: List[str]) -> List[List[float]]:
        try:
            response = client.embeddings.create(model=EMBEDDING_MODEL,
            input=texts)
            return [item["embedding"] for item in response.data]
        except Exception as e:
            logger.error(f"Batch embedding generation failed: {e}")
            raise